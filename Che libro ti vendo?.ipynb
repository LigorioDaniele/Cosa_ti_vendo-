{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scarico il dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://files.grouplens.org/datasets/movielens/ml-latest.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dataset è contenuto all'interno di un'archivio zip, per estrarlo (ipotizzando che tu sia su Ubuntu) ci servirà unzip, installiamolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip ml-latest.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ml-latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I file che ci interessano sono:\n",
    "* **ratings.csv**: che contiene, per ogni riga, id dell'utente, id del film, valutazione da 1.0 a 5.0 e timestamp.\n",
    "* **movies.csv**: che contiene nome e genere dei film associati agli id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inizializzo Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"chelibrotivendo'\") \\\n",
    "        .config(\"spark.executor.cores\", 4) \\\n",
    "        .config(\"spark.sql.pivotMaxValues\", 20000000)\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importo il dataset in un Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"ml-latest/ratings2.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quante sono le transazioni?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048575"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quali sono i 10 prodotti più venduti?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMovies= df.groupBy(\"movieId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|movieId|count|\n",
      "+-------+-----+\n",
      "|    356| 3606|\n",
      "|    318| 3582|\n",
      "|    296| 3410|\n",
      "|    593| 3244|\n",
      "|   2571| 3117|\n",
      "|    260| 3039|\n",
      "|    480| 2791|\n",
      "|    527| 2662|\n",
      "|    110| 2583|\n",
      "|      1| 2573|\n",
      "+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfMovies.count().orderBy(\"count\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tengo solo le colonne che mi servono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[transactionId: int, movieId: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.select('transactionId','movieId')\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot che mi serve per portare la cardinalità su Transaction_Id a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|transactionId|               items|\n",
      "+-------------+--------------------+\n",
      "|          148|[1, 3, 5, 7, 17, ...|\n",
      "|          463|[10, 14, 16, 25, ...|\n",
      "|          471|[1, 10, 32, 47, 5...|\n",
      "|          496|[3, 110, 144, 266...|\n",
      "|          833|[327, 431, 475, 7...|\n",
      "|         1088|[912, 1947, 1989,...|\n",
      "|         1238|[153, 480, 1485, ...|\n",
      "|         1342|    [24, 2723, 2826]|\n",
      "|         1580|[44, 107, 193, 26...|\n",
      "|         1591|[2, 5, 10, 150, 1...|\n",
      "+-------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df_a = df.groupBy('transactionId').agg(F.collect_list('movieId').alias(\"items\"))\n",
    "df_a.cache()\n",
    "df_a.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|     items|freq|\n",
      "+----------+----+\n",
      "|     [356]|3606|\n",
      "|     [318]|3582|\n",
      "|[318, 356]|2223|\n",
      "|     [296]|3410|\n",
      "|[296, 318]|2257|\n",
      "|[296, 356]|2256|\n",
      "|     [593]|3244|\n",
      "|[593, 296]|2288|\n",
      "|[593, 318]|2109|\n",
      "|[593, 356]|2197|\n",
      "|    [2571]|3117|\n",
      "|     [260]|3039|\n",
      "|     [480]|2791|\n",
      "|[480, 356]|2130|\n",
      "|     [527]|2662|\n",
      "|     [110]|2583|\n",
      "|       [1]|2573|\n",
      "|    [1210]|2479|\n",
      "|    [1196]|2444|\n",
      "|     [589]|2404|\n",
      "+----------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+----------+------------------+------------------+\n",
      "|antecedent|consequent|        confidence|              lift|\n",
      "+----------+----------+------------------+------------------+\n",
      "|     [296]|     [318]|0.6618768328445748|1.9460878848461924|\n",
      "|     [296]|     [356]|  0.66158357771261| 1.932279046164506|\n",
      "|     [296]|     [593]|0.6709677419354839| 2.178369993238137|\n",
      "|     [593]|     [296]|0.7053020961775586| 2.178369993238137|\n",
      "|     [593]|     [318]|0.6501233045622689|1.9115294929228965|\n",
      "|     [593]|     [356]|0.6772503082614056|1.9780366740457915|\n",
      "|     [480]|     [356]|0.7631673235399499|2.2289734474550063|\n",
      "|     [356]|     [318]|0.6164725457570716|1.8125876191829922|\n",
      "|     [356]|     [296]|0.6256239600665557|1.9322790461645059|\n",
      "|     [356]|     [593]|0.6092623405435386|1.9780366740457918|\n",
      "|     [318]|     [356]|0.6206030150753769| 1.812587619182992|\n",
      "|     [318]|     [296]|0.6300949190396427|1.9460878848461924|\n",
      "+----------+----------+------------------+------------------+\n",
      "\n",
      "+-------------+--------------------+----------+\n",
      "|transactionId|               items|prediction|\n",
      "+-------------+--------------------+----------+\n",
      "|          148|[1, 3, 5, 7, 17, ...|        []|\n",
      "|          463|[10, 14, 16, 25, ...|        []|\n",
      "|          471|[1, 10, 32, 47, 5...|        []|\n",
      "|          496|[3, 110, 144, 266...|     [356]|\n",
      "|          833|[327, 431, 475, 7...|        []|\n",
      "|         1088|[912, 1947, 1989,...|        []|\n",
      "|         1238|[153, 480, 1485, ...|     [356]|\n",
      "|         1342|    [24, 2723, 2826]|        []|\n",
      "|         1580|[44, 107, 193, 26...|        []|\n",
      "|         1591|[2, 5, 10, 150, 1...|     [356]|\n",
      "|         1645|[1, 10, 19, 39, 4...|        []|\n",
      "|         1829|[163, 168, 186, 3...|        []|\n",
      "|         1959|[10, 34, 47, 110,...|     [296]|\n",
      "|         2122|[48, 50, 158, 236...|[356, 296]|\n",
      "|         2142|[318, 1387, 4306,...|[356, 296]|\n",
      "|         2366|[50, 216, 597, 90...|        []|\n",
      "|         2659|[10, 28, 50, 213,...|     [356]|\n",
      "|         2866|[32, 318, 527, 15...|[356, 296]|\n",
      "|         3175|[69, 227, 262, 51...|        []|\n",
      "|         3749|[110, 111, 296, 3...|     [318]|\n",
      "+-------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.2, minConfidence=0.6)\n",
    "model = fpGrowth.fit(df_a)\n",
    "model.freqItemsets.show()\n",
    "model.associationRules.show()\n",
    "model.transform(df_a).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
